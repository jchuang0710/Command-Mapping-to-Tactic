{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step1. Generate Data"
      ],
      "metadata": {
        "id": "liOMjsWilqJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 抓 linux commond"
      ],
      "metadata": {
        "id": "bv5nqNzLTeLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import collections\n",
        "import re\n",
        "\n",
        "section_base_url = \"https://man7.org/linux/man-pages/dir_section_{}.html\"\n",
        "\n",
        "# Define a function to extract command names and descriptions\n",
        "def get_commands_from_section(section):\n",
        "    url = section_base_url.format(section)\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        command_entries = soup.find_all('td', valign=\"top\")\n",
        "\n",
        "        posix_commands = []\n",
        "        gnu_commands = []\n",
        "\n",
        "        for entry in command_entries:\n",
        "            commands_a = entry.find_all('a')\n",
        "            for command in commands_a:\n",
        "                if command != None:\n",
        "                    text = command.text\n",
        "                    if text == \"intro(1)\" or text == \"intro(8)\": #remove intro\n",
        "                        continue\n",
        "                    if \"(1p)\" in text: #posix\n",
        "                        text = text.replace(\"(1p)\", \"\")\n",
        "                        posix_commands.append(text)\n",
        "                    elif \"(1)\" in text:\n",
        "                        text = text.replace(\"(1)\", \"\")\n",
        "                        gnu_commands.append(text)\n",
        "                    elif \"(8)\" in text:\n",
        "                        text = text.replace(\"(8)\", \"\")\n",
        "                        gnu_commands.append(text)\n",
        "\n",
        "        # If a command is in both posix page and gnu page , remove the posix one\n",
        "        same_commands = list(set(posix_commands).intersection(gnu_commands))\n",
        "        posix_commands = [i for i in posix_commands if i not in same_commands]\n",
        "\n",
        "        posix_commands.sort()\n",
        "        gnu_commands.sort()\n",
        "\n",
        "        all_commands = posix_commands + gnu_commands\n",
        "\n",
        "        return [posix_commands, gnu_commands, all_commands]\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to fetch section {section}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "section1_commands = get_commands_from_section(1)\n",
        "section8_commands = get_commands_from_section(8)\n",
        "\n",
        "#remove tcpdump in section8\n",
        "same_commands = list(set(section1_commands[2]).intersection(section8_commands[2]))\n",
        "for i in [0, 1, 2]:\n",
        "    section8_commands[i] = [j for j in section8_commands[i] if j not in same_commands]\n",
        "\n",
        "print(f\"Section1: {len(section1_commands[2])}\\n\")\n",
        "print(f\"Section8: {len(section8_commands[2])}\\n\")\n",
        "print(f\"Section1 + Section8: {len(section1_commands[2]) + len(section8_commands[2])}\\n\")\n",
        "\n",
        "import json\n",
        "\n",
        "man_page_posix_url = \"https://man7.org/linux/man-pages/man{}/{}.{}p.html\"\n",
        "man_page_gnu_url = \"https://man7.org/linux/man-pages/man{}/{}.{}.html\"\n",
        "\n",
        "def get_command_description_from_command(section, command, is_posix):\n",
        "    if is_posix:\n",
        "        url = man_page_posix_url.format(section, command, section)\n",
        "    else:\n",
        "        url = man_page_gnu_url.format(section, command, section)\n",
        "    while True:\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "        except:\n",
        "            continue\n",
        "        break\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        name_a = soup.find('a', id=\"NAME\")\n",
        "        name = None\n",
        "        if name_a is not None:\n",
        "            name_h2 = name_a.parent\n",
        "            name = name_h2.findNext('pre').text\n",
        "            name = name.strip()\n",
        "            name = name.replace(\"\\n\", \"\")\n",
        "            name = re.sub(r\"([\\n])|([\\ ]{2,})\", \"\", name)\n",
        "\n",
        "\n",
        "        description = None\n",
        "        description_a = soup.find('a', id=\"DESCRIPTION\")\n",
        "        if description_a is not None:\n",
        "            description_h2 = description_a.parent\n",
        "            description = description_h2.findNext('pre').text\n",
        "            description = description.strip()\n",
        "            description = description.replace(\"\\\\\",\"//\").replace(r\"\\u2014\",r\"-\").replace('\\u2022','.')\n",
        "            description = re.sub(r\"([\\n])|([\\ ]{2,})\", \"\", description)\n",
        "\n",
        "\n",
        "        return {'name':name, 'description':description}\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to fetch command {command}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def get_descriptions(section, commands, is_posix):\n",
        "    command_list = []\n",
        "    for command in commands:\n",
        "        print(f'current fetch command: {command}\\n')\n",
        "        result = get_command_description_from_command(section, command, is_posix)\n",
        "        if result != None:\n",
        "            result['command'] = command\n",
        "            result['section'] = section\n",
        "            command_list.append(result)\n",
        "    return command_list\n",
        "\n",
        "res1 = get_descriptions(1, section1_commands[0], True)\n",
        "res2 = get_descriptions(1, section1_commands[1], False)\n",
        "res3 = get_descriptions(8, section8_commands[0], True)\n",
        "res4 = get_descriptions(8, section8_commands[1], False)\n",
        "\n",
        "res = res1+res2+res3+res4\n",
        "json = json.dumps(res, sort_keys=True, indent=4)\n",
        "print(json)\n",
        "\n",
        "f = open(\"result.json\", \"w+\")\n",
        "f.write(json)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "q3IuZveZXsj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 讀取檔案\n",
        "* 把抓到的commod(result.json)放到google 雲端硬碟的root\n",
        "* 把[github]((https://github.com/mitre/cti/tree/master/enterprise-attack/x-mitre-tactic)) 的 14個tactic.json 放到google 雲端硬碟root/tactic"
      ],
      "metadata": {
        "id": "gVv_5mjUF9Cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "讀取 linux commond"
      ],
      "metadata": {
        "id": "f6Hlq8U8TZud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "iAA8RZekfAT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yxsbUps_QKm",
        "outputId": "e60ad460-1bbf-4e0f-c1ae-63724305e30d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dirPath = r\"drive/MyDrive/\"\n",
        "f = open(dirPath + 'result.json')\n",
        "data = json.load(f)\n",
        "\n",
        "# Use pd.json_normalize to convert the JSON to a DataFrame\n",
        "commond = pd.json_normalize(data, meta=['command', 'description', 'name', 'section'])\n",
        "\n",
        "# Rename the columns for clarity\n",
        "commond.columns = ['command', 'description', 'name', 'section']\n",
        "\n",
        "# Display the DataFrame\n",
        "print(commond)\n",
        "commond = pd.DataFrame(commond)\n",
        "commond = technique.dropna()\n",
        "commond.reset_index(drop=True,inplace=True)\n",
        "commond"
      ],
      "metadata": {
        "id": "N8zll0wjqNDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "讀取 tactic"
      ],
      "metadata": {
        "id": "fJtODB-UTEjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dirPath = r\"drive/MyDrive/tactic\"\n",
        "\n",
        "tmp = []\n",
        "for f in os.listdir(dirPath):\n",
        "  if pathlib.Path(f).suffix == \".json\":\n",
        "    data = json.load(open(os.path.join(dirPath, f)))\n",
        "    tmp.append([data['objects'][0]['name'], data['objects'][0]['description']])\n",
        "\n",
        "tatic = pd.DataFrame(tmp)\n",
        "tatic.columns = ['name', 'description']\n",
        "print(tatic)"
      ],
      "metadata": {
        "id": "rhkLD5HVN07A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 抓 Technique"
      ],
      "metadata": {
        "id": "Td_LvUuNBALL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url = \"https://attack.mitre.org/techniques/enterprise/\"\n",
        "\n",
        "tables = pd.read_html(url)\n",
        "\n",
        "len(tables)\n",
        "technique = tables[0]\n",
        "\n",
        "technique = technique.rename(columns={\"Description\": \"description\", \"Name\": \"name\"})\n",
        "technique = pd.DataFrame(technique)\n",
        "technique = technique.dropna()\n",
        "technique.reset_index(drop=True,inplace=True)\n",
        "technique"
      ],
      "metadata": {
        "id": "_SbX-xrTA_f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Technique map to Tactic"
      ],
      "metadata": {
        "id": "HZKGgzrtsCew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url = \"https://attack.mitre.org/matrices/enterprise/\"\n",
        "\n",
        "tables = pd.read_html(url)\n",
        "\n",
        "len(tables)\n",
        "matrix = tables[0]\n",
        "\n",
        "matrix = pd.DataFrame(matrix)"
      ],
      "metadata": {
        "id": "bZtWGeSusBpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step2. Pre-Processing(SnowballStemmer)"
      ],
      "metadata": {
        "id": "MbsOILc9FO1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import numpy as np\n",
        "\n",
        "pd.options.display.max_columns = 30\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "sVaBwZ8d7gMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    phrase = re.sub(r\"early access review\", \"early access review \", phrase)\n",
        "    phrase = re.sub(r\"\\+\", \" + \", phrase)\n",
        "    phrase = re.sub(r\"\\-\", \" - \", phrase)\n",
        "    phrase = re.sub(r\"/10\", \"/10 \", phrase)\n",
        "    phrase = re.sub(r\"10/\", \" 10/\", phrase)\n",
        "    return phrase"
      ],
      "metadata": {
        "id": "CRbY-qrDFOMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer(\"english\")\n",
        "def stemming_tokenizer(str_input):\n",
        "  words = re.sub(r\"[^a-zA-Z]{2,}\", \" \", str_input).lower().split()\n",
        "  words = [stemmer.stem(word) for word in words]\n",
        "  return \" \".join(words)"
      ],
      "metadata": {
        "id": "clpxhdyTG3I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_reviews(lst):\n",
        "    # remove URL links (httpxxx)\n",
        "    lst = np.vectorize(remove_pattern)(lst, \"https?://[A-Za-z0-9./]*\")\n",
        "    # remove special characters, numbers, punctuations (except for #)\n",
        "    lst = np.core.defchararray.replace(lst, \"[^a-zA-Z]\", \" \")\n",
        "    # remove amp with and\n",
        "    lst = np.vectorize(replace_pattern)(lst, \"amp\", \"and\")\n",
        "    # remove hashtags\n",
        "    lst = np.vectorize(remove_pattern)(lst, \"#[A-Za-z0-9]+\")\n",
        "    lst = np.vectorize(remove_pattern)(lst, \"#[\\w]*\")\n",
        "    return lst\n",
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "    return input_txt\n",
        "def replace_pattern(input_txt, pattern, replace_text):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, replace_text, input_txt)\n",
        "    return input_txt"
      ],
      "metadata": {
        "id": "3R1S2CvtFW3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commond.loc[4, 'description']"
      ],
      "metadata": {
        "id": "vCZhVEQ0HWnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying pre-processing to user reviews\n",
        "def preprocessing(df):\n",
        "  text2 = clean_reviews(list(df['description'].astype('str')))\n",
        "  text3 = [ta.lower() for ta in text2]\n",
        "  text4 = [''.join([i if ord(i) < 128 else ' ' for i in t]) for t in text3]\n",
        "  text5 = [decontracted(u) for u in text4]\n",
        "  text6 = [stemming_tokenizer(u) for u in text5]\n",
        "  return text6"
      ],
      "metadata": {
        "id": "wM_0Xj_KFdyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commond.loc[4, 'description']"
      ],
      "metadata": {
        "id": "eOn_S6geHLKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fixedCommond = preprocessing(commond)\n",
        "fixedTatic = preprocessing(tatic)\n",
        "#fixedTechnique = preprocessing(technique)"
      ],
      "metadata": {
        "id": "tqMGHdInHp5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step3. Mapping(Bag of Word、TF IDF)"
      ],
      "metadata": {
        "id": "HEfMkTqmlTz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "RaSZ8f0iDiMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://attack.mitre.org/tactics/enterprise/\"\n",
        "\n",
        "tables = pd.read_html(url)\n",
        "\n",
        "#len(tables)\n",
        "matrix = tables[0]\n",
        "matrix\n",
        "#matrix = matrix.rename(columns={\"Description\": \"description\", \"Name\": \"name\"})\n",
        "matrix = pd.DataFrame(matrix)\n",
        "matrix = matrix.dropna()\n",
        "matrix.reset_index(drop=True,inplace=True)\n",
        "\n",
        "dic = {}\n",
        "for i in range(len(technique)):\n",
        "  dic[technique.loc[i, 'name']] = []\n",
        "\n",
        "url = \"https://attack.mitre.org/tactics/\"\n",
        "for i in range(len(matrix)):\n",
        "  tables2 = pd.read_html(url + matrix.loc[i]['ID'] + '/')\n",
        "  matrix2 = tables2[0]\n",
        "  matrix2 = pd.DataFrame(matrix2)\n",
        "  matrix2 = matrix2.dropna()\n",
        "  matrix2.reset_index(drop=True,inplace=True)\n",
        "  for j in range(len(matrix2)):\n",
        "    dic[matrix2.loc[j]['Name']].append(matrix.loc[i]['Name'])\n",
        "\n",
        "print(dic)\n",
        "json2 = json.dumps(dic)\n",
        "print(json2)\n",
        "\n",
        "f = open(\"mapTech2Tactic.json\", \"w+\")\n",
        "f.write(json2)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "sqpuahx8nmKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#計算文件與文件的cosine similarity\n",
        "def similarity(vec):\n",
        "  mapping = {}\n",
        "  for i in range(len(fixedTatic), 2631):\n",
        "    max = 0;\n",
        "    max_id = 0;\n",
        "    for j in range(0, len(fixedTatic)):\n",
        "      score = cosine_similarity(vec[i], vec[j])\n",
        "      if(max < score):\n",
        "        max = score\n",
        "        max_id = j\n",
        "    if commond.loc[i, 'name']:\n",
        "      name = commond.loc[i, 'name'].split(' ')\n",
        "    else:\n",
        "      name = [commond.loc[i, 'name'],0]\n",
        "    if max != 0:\n",
        "      print(name[0], ': is mapping to', technique.loc[max_id, 'name'])\n",
        "      mapping[name[0]] = technique.loc[max_id, 'name']\n",
        "    else:\n",
        "      print(name[0], ': is not mapping to any technique')\n",
        "      mapping[name[0]] = ''\n",
        "  '''\n",
        "  with open(\"sample.json\", \"w\") as outfile:\n",
        "    json.dumps(mapping, outfile)\n",
        "  '''\n",
        "  json2 = json.dumps(mapping)\n",
        "  print(json2)\n",
        "\n",
        "  f = open(\"sample.json\", \"w+\")\n",
        "  f.write(json2)\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "JMtcd-_Ri1oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words"
      ],
      "metadata": {
        "id": "0e9a8P_8jiQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vect = CountVectorizer(analyzer='word', stop_words = \"english\")\n",
        "countdf_user_review = count_vect.fit_transform(fixedTatic + fixedCommond)\n",
        "print(\"All tags are:\")\n",
        "print(count_vect.get_feature_names_out())\n",
        "print(\"Matrix looks like\")\n",
        "print(countdf_user_review.shape)\n",
        "print(countdf_user_review.toarray())"
      ],
      "metadata": {
        "id": "lDCtcA4x7nyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity(countdf_user_review)"
      ],
      "metadata": {
        "id": "Z2-YRPbBjAyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BoW for N-gram"
      ],
      "metadata": {
        "id": "fBcU9Bs8jr7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count Vectorizer for N-grams\n",
        "count_vect2 = CountVectorizer(analyzer='word', ngram_range=(2,3), stop_words = \"english\")\n",
        "countdf_user_review2= count_vect2.fit_transform(fixedTatic + fixedCommond)\n",
        "print(\"All tags are:\")\n",
        "print(count_vect2.get_feature_names_out())\n",
        "print(\"Matrix looks like\")\n",
        "print(countdf_user_review2.shape)\n",
        "print(countdf_user_review2.toarray())"
      ],
      "metadata": {
        "id": "gcPu7-Xejmri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity(countdf_user_review2)"
      ],
      "metadata": {
        "id": "d4TuFSz1i9Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF"
      ],
      "metadata": {
        "id": "A1C4AtJqsEnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word level Tf-Idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', stop_words = \"english\")\n",
        "tfidf_user_review = tfidf_vect.fit_transform(fixedTatic + fixedCommond)\n",
        "print(\"All tags are:\")\n",
        "print(tfidf_vect.get_feature_names_out())\n",
        "print(\"Matrix looks like\")\n",
        "print(tfidf_user_review.shape)\n",
        "print(tfidf_user_review.toarray())"
      ],
      "metadata": {
        "id": "6aGkQy7xsHkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity(tfidf_user_review)"
      ],
      "metadata": {
        "id": "NkY9HiN9jzT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tf-Idf for N-grams"
      ],
      "metadata": {
        "id": "WYN1HExVsXFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tf-Idf for N-grams\n",
        "tfidf_vect2 = TfidfVectorizer(analyzer='word', ngram_range=(2,3), stop_words = \"english\")\n",
        "tfidf_user_review2 = tfidf_vect2.fit_transform(fixedTatic + fixedCommond)\n",
        "print(\"All tags are:\")\n",
        "print(tfidf_vect2.get_feature_names_out())\n",
        "print(\"Matrix looks like\")\n",
        "print(tfidf_user_review2.shape)\n",
        "print(tfidf_user_review2.toarray())"
      ],
      "metadata": {
        "id": "PW9HKYBDsUF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity(tfidf_user_review2)"
      ],
      "metadata": {
        "id": "N4-iJ_oTvESK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}